{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m alpha \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m loss:\n\u001b[0;32m----> 6\u001b[0m     new_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(_ \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39malpha)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m      7\u001b[0m new_loss\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss = [1,2,3,4]\n",
    "new_loss = []\n",
    "alpha =0.5\n",
    "for _ in loss:\n",
    "    new_loss.append(sum(_ + torch.tensor(loss)*alpha).item())\n",
    "new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'iter'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model1 = torch.load(\"/Applications/codingforlife/LabNLP/LED/sharpseq/checkpoint/model.1\", map_location=torch.device('cpu'))\n",
    "model1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1['iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Applications/codingforlife/LabNLP/LED/sharpseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from utils.options import Config, PERM\n",
    "def parse_arguments():\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # define_arguments(parser)\n",
    "    # args = parser.parse_args()\n",
    "    args = Config(train_epoch=2,\n",
    "                  load_model=\"./checkpoint\",\n",
    "                  json_root=\"./\",\n",
    "                  feature_root='/Applications/codingforlife/LabNLP/LED/data/features',\n",
    "                  stream_file='/Applications/codingforlife/LabNLP/LED/data/MAVEN/streams.json',\n",
    "                  log_dir='./log')\n",
    "    \n",
    "    args.log = os.path.join(args.log_dir, \"logfile.log\")\n",
    "\n",
    "    if (not args.test_only) and os.path.exists(args.log_dir):\n",
    "        existing_logs = glob.glob(os.path.join(args.log_dir, \"*\"))\n",
    "        for _t in existing_logs:\n",
    "            if 'exp.log' not in _t and not _t.endswith('.py'):\n",
    "                os.remove(_t)\n",
    "    print('Dump name space')\n",
    "    if not os.path.exists(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "    with open(f'{args.log_dir}/options.json', 'w') as f:\n",
    "        print(f'{args.log_dir}/options.json')\n",
    "        \n",
    "        json.dump(vars(args), f, ensure_ascii=False)\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump name space\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatastream\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_stage_loaders, get_stage_loaders_n\n\u001b[1;32m      3\u001b[0m opts \u001b[38;5;241m=\u001b[39m parse_arguments()\n\u001b[1;32m      4\u001b[0m perm_id \u001b[38;5;241m=\u001b[39m opts\u001b[38;5;241m.\u001b[39mperm_id\n",
      "File \u001b[0;32m/Applications/codingforlife/LabNLP/LED/sharpseq/utils/datastream.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_arguments,  PERM\n\u001b[0;32m---> 12\u001b[0m opts \u001b[38;5;241m=\u001b[39m \u001b[43mparse_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m PERM_ID \u001b[38;5;241m=\u001b[39m opts\u001b[38;5;241m.\u001b[39mperm_id\n",
      "File \u001b[0;32m/Applications/codingforlife/LabNLP/LED/sharpseq/utils/options.py:219\u001b[0m, in \u001b[0;36mparse_arguments\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDump name space\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(args\u001b[38;5;241m.\u001b[39mlog_dir):\n\u001b[0;32m--> 219\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/options.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/options.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle'"
     ]
    }
   ],
   "source": [
    "from utils.datastream import get_stage_loaders, get_stage_loaders_n\n",
    "\n",
    "opts = parse_arguments()\n",
    "perm_id = opts.perm_id\n",
    "dataset_id = 0\n",
    "streams = json.load(open(opts.stream_file))\n",
    "streams = [streams[t] for t in PERM[perm_id]]\n",
    "\n",
    "\n",
    "loaders, exemplar_loaders, stage_labels, label2id = get_stage_loaders(root=opts.json_root,\n",
    "    feature_root=opts.feature_root,\n",
    "    batch_size=opts.batch_size,\n",
    "    streams=streams,\n",
    "    num_workers=0,\n",
    "    dataset=dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.optimizer import AdamW\n",
    "from torch.optim import SGD, Adagrad\n",
    "from utils.options import parse_arguments, PERM\n",
    "from utils.datastream import get_stage_loaders, get_stage_loaders_n\n",
    "from utils.worker import Worker\n",
    "from models.nets import LInEx, BIC, ICARL\n",
    "\n",
    "summary = None\n",
    "torch.set_num_threads(20)\n",
    "def add_summary_value(writer, key, value, iteration):\n",
    "    return\n",
    "\n",
    "def by_class(preds, labels, learned_labels=None):\n",
    "    match = (preds == labels).float()\n",
    "    nlabels = max(torch.max(labels).item(), torch.max(preds).item())\n",
    "    bc = {}\n",
    "\n",
    "    ag = 0; ad = 0; am = 0\n",
    "    for label in range(1, nlabels+1):\n",
    "        lg = (labels==label); ld = (preds==label)\n",
    "        lr = torch.sum(match[lg]) / torch.sum(lg.float())\n",
    "        lp = torch.sum(match[ld]) / torch.sum(ld.float())\n",
    "        lf = 2 * lr * lp / (lr + lp)\n",
    "        if torch.isnan(lf):\n",
    "            bc[label] = (0, 0, 0)\n",
    "        else:\n",
    "            bc[label] = (lp.item(), lr.item(), lf.item())\n",
    "        if learned_labels is not None and label in learned_labels:\n",
    "            ag += lg.float().sum()\n",
    "            ad += ld.float().sum()\n",
    "            am += match[lg].sum()\n",
    "    if learned_labels is None:\n",
    "        ag = (labels!=0); ad = (preds!=0)\n",
    "        sum_ad = torch.sum(ag.float())\n",
    "        if sum_ad == 0:\n",
    "            ap = ar = 0\n",
    "        else:\n",
    "            ar = torch.sum(match[ag]) / torch.sum(ag.float())\n",
    "            ap = torch.sum(match[ad]) / torch.sum(ad.float())\n",
    "    else:\n",
    "        if ad == 0:\n",
    "            ap = ar = 0\n",
    "        else:\n",
    "            ar = am / ag; ap = am / ad\n",
    "    if ap == 0:\n",
    "        af = ap = ar = 0\n",
    "    else:\n",
    "        af = 2 * ar * ap / (ar + ap)\n",
    "        af = af.item(); ar = ar.item(); ap = ap.item()\n",
    "    return bc, (ap, ar, af)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    opts = parse_arguments()\n",
    "    torch.manual_seed(opts.seed)\n",
    "    np.random.seed(opts.seed)\n",
    "\n",
    "    dataset_id = 0\n",
    "\n",
    "    perm_id = opts.perm_id\n",
    "    if opts.setting == \"classic\":\n",
    "        streams = json.load(open(opts.stream_file))\n",
    "        streams = [streams[t] for t in PERM[perm_id]]\n",
    "        loaders, exemplar_loaders, stage_labels, label2id = get_stage_loaders(root=opts.json_root,\n",
    "            feature_root=opts.feature_root,\n",
    "            batch_size=opts.batch_size,\n",
    "            streams=streams,\n",
    "            num_workers=0,\n",
    "            dataset=dataset_id)\n",
    "\n",
    "    model = LInEx(\n",
    "        input_dim=opts.input_dim,\n",
    "        hidden_dim=opts.hidden_dim,\n",
    "        max_slots=opts.max_slots,\n",
    "        init_slots=max(stage_labels[0])+1 if not opts.test_only else max(stage_labels[-1])+1,\n",
    "        device=torch.device(torch.device(f'cuda:{opts.gpu}' if torch.cuda.is_available() and (not opts.no_gpu) else 'cpu')),\n",
    "        dropout_type = opts.dropout,\n",
    "        p = opts.p\n",
    "        )\n",
    "    param_groups = [\n",
    "        {\"params\": [param for name, param in model.named_parameters() if param.requires_grad and 'correction' not in name],\n",
    "        \"lr\":opts.learning_rate,\n",
    "        \"weight_decay\": opts.decay,\n",
    "        \"betas\": (0.9, 0.999)}\n",
    "        ]\n",
    "    optimizer = AdamW(params=param_groups)\n",
    "    optimizer_correction = None\n",
    "\n",
    "    worker = Worker(opts)\n",
    "    worker._log(str(opts))\n",
    "    worker._log(str(label2id))\n",
    "    if opts.test_only:\n",
    "        worker.load(model)\n",
    "    best_dev = best_test = None\n",
    "    collect_stats = \"accuracy\"\n",
    "    collect_outputs = {\"prediction\", \"label\"}\n",
    "    termination = False\n",
    "    patience = opts.patience\n",
    "    no_better = 0\n",
    "    loader_id = 0\n",
    "    total_epoch = 0\n",
    "    none_mul = 4\n",
    "    learned_labels = set(stage_labels[0])\n",
    "    dev_metrics = None\n",
    "    test_metrics = None\n",
    "    print(\"Finetune and Generate\",opts.finetune, opts.generate)\n",
    "    while not termination:\n",
    "        if not opts.test_only:\n",
    "            if opts.skip_first and loader_id == 0:\n",
    "                worker.load(model, optimizer, path=opts.load_first, strict=opts.balance!='bic')\n",
    "                total_epoch += worker.epoch\n",
    "            elif opts.skip_second and loader_id == 1:\n",
    "                worker.load(model, optimizer, path=opts.load_second, strict=opts.balance!='bic')\n",
    "                total_epoch += worker.epoch\n",
    "            else:\n",
    "                if opts.finetune:\n",
    "                    train_loss = lambda batch: model.forward(batch)\n",
    "                else:\n",
    "                    train_loss = lambda batch: model.forward(batch, contrastive=opts.contrastive, return_loss_list=opts.mul_task, generate=opts.generate, sample_size=opts.sample_size,\n",
    "                                                            exemplar=True, exemplar_distill=True, distill=True, tau=0.5,\n",
    "                                                            feature_distill=opts.features_distill, hyer_distill=opts.hyer_distill, mul_distill=opts.mul_distill,\n",
    "                                                            lambda_coef=1e-2)\n",
    "                epoch_loss, epoch_metric = worker.run_one_epoch(\n",
    "                        model=model,\n",
    "                        f_loss=train_loss,\n",
    "                        loader=loaders[loader_id],\n",
    "                        split=\"train\",\n",
    "                        optimizer=optimizer,\n",
    "                        collect_stats=collect_stats,\n",
    "                        prog=loader_id)\n",
    "                \n",
    "                total_epoch += 1\n",
    "                # if opts.debug:\n",
    "                #     import pdb\n",
    "                #     pdb.set_trace()\n",
    "                for output_log in [print, worker._log]:\n",
    "                    output_log(\n",
    "                        f\"Epoch {worker.epoch:3d}  Train Loss {epoch_loss} {epoch_metric}\")\n",
    "                # torch.save(model.state_dict(), )\n",
    "        else:\n",
    "            learned_labels = set([t for stream in stage_labels for t in stream])\n",
    "            termination = True\n",
    "\n",
    "\n",
    "        score_fn = model.score\n",
    "\n",
    "        dev_loss, dev_metrics = worker.run_one_epoch(\n",
    "            model=model,\n",
    "            f_loss=score_fn,\n",
    "            loader=loaders[-2],\n",
    "            split=\"dev\",\n",
    "            collect_stats=collect_stats,\n",
    "            collect_outputs=collect_outputs)\n",
    "        dev_outputs = {k: torch.cat(v, dim=0) for k,v in worker.epoch_outputs.items()}\n",
    "        dev_scores, (dev_p, dev_r, dev_f) = by_class(dev_outputs[\"prediction\"], dev_outputs[\"label\"], learned_labels=learned_labels)\n",
    "        dev_class_f1 = {k: dev_scores[k][2] for k in dev_scores}\n",
    "        for k,v in dev_class_f1.items():\n",
    "            add_summary_value(summary, f\"dev_class_{k}\", v, total_epoch)\n",
    "        dev_metrics = dev_f\n",
    "        for output_log in [print, worker._log]:\n",
    "            output_log(\n",
    "                f\"Epoch {worker.epoch:3d}:  Dev {dev_metrics}\"\n",
    "            )\n",
    "        # if worker.epoch == opts.train_epoch:\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        if opts.test_only:\n",
    "            frequency = {}\n",
    "            for loader in loaders[:-2]:\n",
    "                indices = loader.dataset.label2index\n",
    "                for label in indices.keys():\n",
    "                    if label != 0:\n",
    "                        frequency[label] = indices[label][1] - indices[label][0]\n",
    "            with open(\"/kaggle/working/sharpseq/data/MAVEN/label2id.json\") as fp:\n",
    "                name2label = json.load(fp)\n",
    "                label2name = {v:k for k,v in name2label.items()}\n",
    "            id2label = {v:k for k,v in label2id.items()}\n",
    "            sf = [(frequency[l], label2name[id2label[l] ], dev_class_f1[l], test_class_f1[l]) for l in frequency]\n",
    "            sf.sort(key=lambda t:t[0])\n",
    "            print(\"macro:\", sum([t[3] for t in sf]) / len(sf))\n",
    "\n",
    "        if not opts.test_only:\n",
    "            if best_dev is None or dev_metrics > best_dev:\n",
    "                test_loss, test_metrics = worker.run_one_epoch(\n",
    "                    model=model,\n",
    "                    f_loss=score_fn,\n",
    "                    loader=loaders[-1],\n",
    "                    split=\"test\",\n",
    "                    collect_stats=collect_stats,\n",
    "                    collect_outputs=collect_outputs)\n",
    "                test_outputs = {k: torch.cat(v, dim=0) for k,v in worker.epoch_outputs.items()}\n",
    "                # if opts.debug:\n",
    "                #     import pdb\n",
    "                #     pdb.set_trace()\n",
    "                # torch.save(test_outputs, f\"/kaggle/working/log/{os.path.basename(opts.load_model)}.output\")\n",
    "                torch.save(test_outputs, f\"/kaggle/working/log/test_outputs.output\")\n",
    "                test_scores, (test_p, test_r, test_f) = by_class(test_outputs[\"prediction\"], test_outputs[\"label\"], learned_labels=learned_labels)\n",
    "                test_class_f1 = {k: test_scores[k][2] for k in test_scores}\n",
    "                for k,v in test_class_f1.items():\n",
    "                    add_summary_value(summary, f\"test_class_{k}\", v, total_epoch)\n",
    "                test_metrics = test_f\n",
    "                for output_log in [print, worker._log]:\n",
    "                    output_log(\n",
    "                        f\"Epoch {worker.epoch:3d}: Test {test_metrics}\"\n",
    "                    )\n",
    "                best_dev = dev_metrics\n",
    "                worker.save(model, optimizer, postfix=str(loader_id))\n",
    "                best_test = test_metrics\n",
    "                no_better = 0\n",
    "            else:\n",
    "                no_better += 1\n",
    "\n",
    "            print(f\"patience: {no_better} / {patience}\")\n",
    "            try:\n",
    "                with open(\"/kaggle/working/log/alpha.txt\", \"a+\") as f:\n",
    "                    f.write(torch.exp(model.input_map[3].log_alpha).__str__())\n",
    "                    f.write(\"\\n\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if (no_better == patience) or (worker.epoch == worker.train_epoch) or (opts.skip_first and loader_id == 0) or (opts.skip_second and loader_id == 1):\n",
    "                \n",
    " \n",
    "                loader_id += 1\n",
    "                #model.id2task[0] = loader_id\n",
    "                no_better = 0\n",
    "                worker.load(model, optimizer, path=os.path.join(opts.log_dir, f\"{worker.save_model}.{loader_id-1}\"))\n",
    "                if not opts.finetune:\n",
    "                    print(\"setting train exemplar for learned classes\")\n",
    "                    #model.set_clusters(exemplar_loaders[loader_id-1])\n",
    "                    model.set_exemplar(exemplar_loaders[loader_id-1], generate_ratio=opts.generate_ratio, generate=opts.generate, center_ratio=opts.center_ratio, mode=opts.mode, num_clusters=opts.clusters)\n",
    "                    print(\"SET EXAMPLAR\")\n",
    "                model.set_class()\n",
    "                model.lm_head.expand(model.nslots)\n",
    "\n",
    "                if not opts.finetune:\n",
    "                    model.set_history()\n",
    "                    print(\"SET HISTORY\")\n",
    "                for output_log in [print, worker._log]:\n",
    "                    output_log(f\"BEST DEV {loader_id-1}: {best_dev if best_dev is not None else 0}\")\n",
    "                    output_log(f\"BEST TEST {loader_id-1}: {best_test if best_test is not None else 0}\")\n",
    "                if loader_id == len(loaders) - 2:\n",
    "                    termination = True\n",
    "                else:\n",
    "                    learned_labels = learned_labels.union(set(stage_labels[loader_id]))\n",
    "                    if opts.kt:\n",
    "                        next_exemplar = model.set_exemplar(exemplar_loaders[loader_id], mode=opts.kt_mode, output_only=True)\n",
    "                        next_frequency = {}\n",
    "                        indices = loaders[loader_id].dataset.label2index\n",
    "                        for label in stage_labels[loader_id]:\n",
    "                            if label != 0:\n",
    "                                next_frequency[label] = indices[label][1] - indices[label][0]\n",
    "                        if opts.kt2:\n",
    "                            next_inits = model.initialize2(\n",
    "                                exemplar=next_exemplar,\n",
    "                                ninstances=next_frequency,\n",
    "                                gamma=opts.kt_gamma,\n",
    "                                tau=opts.kt_tau,\n",
    "                                alpha=opts.kt_alpha,\n",
    "                                delta=opts.kt_delta)\n",
    "                        else:\n",
    "                            next_inits = model.initialize(\n",
    "                                exemplar=next_exemplar,\n",
    "                                ninstances=next_frequency,\n",
    "                                gamma=opts.kt_gamma,\n",
    "                                tau=opts.kt_tau,\n",
    "                                alpha=opts.kt_alpha)\n",
    "                        torch.save(model.outputs[\"new2old\"], os.path.join(opts.log_dir, f\"{loader_id}_to_{loader_id-1}\"))\n",
    "                        model.extend(next_inits)\n",
    "                        assert model.nslots == max(learned_labels) + 1\n",
    "                    else:\n",
    "                        model.nslots = max(learned_labels) + 1\n",
    "                worker.epoch = 0\n",
    "                best_dev = None; best_test = None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
